{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiwari-arpit/nlp/blob/main/HegelBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URrSjsPacGuw"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf_fShHVPMHA",
        "outputId": "7da0245f-7114-4b0c-9e35-1369bd60e86f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Transformers in /opt/conda/lib/python3.11/site-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from Transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from Transformers) (0.28.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from Transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from Transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from Transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from Transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from Transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from Transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from Transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from Transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->Transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->Transformers) (4.12.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->Transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->Transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->Transformers) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->Transformers) (2024.12.14)\n",
            "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (0.34.2)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.4.1.post100)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.28.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.5.2)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Using cached accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
            "Installing collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.34.2\n",
            "    Uninstalling accelerate-0.34.2:\n",
            "      Successfully uninstalled accelerate-0.34.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
            "autogluon-multimodal 1.2 requires accelerate<1.0,>=0.34.0, but you have accelerate 1.4.0 which is incompatible.\n",
            "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
            "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
            "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
            "autogluon-timeseries 1.2 requires accelerate<1.0,>=0.34.0, but you have accelerate 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Transformers\n",
        "!pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3Ljn8GwPhFW"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-LpTqBhcVWj"
      },
      "source": [
        "**Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqAgqIxdQCGW"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "\n",
        "file_contents = []\n",
        "for path in paths:\n",
        "  try:\n",
        "    with open(path,\"r\",encoding=\"utf-8\",errors=\"replace\") as file:\n",
        "      file_contents.append(file.read())\n",
        "  except Exception as e:\n",
        "    print(f\"Error reading file {path}: {e}\")\n",
        "\n",
        "text = \"\\n\".join(file_contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvdi_qHDcn4R"
      },
      "source": [
        "**Training Tokenizer and saving files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUP50AcwQwPb",
        "outputId": "901ba73c-0dbc-4c65-bffc-4892166d35f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "special_tokens = [\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"]\n",
        "\n",
        "tokenizer.train_from_iterator([text],vocab_size=52_000,min_frequency=2, special_tokens = special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HLSz49ZRdrq",
        "outputId": "44e24dab-4fa1-490c-9dd6-6d191b1f71df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['HegelBERT/vocab.json', 'HegelBERT/merges.txt']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "token_dir = './HegelBERT'\n",
        "if not os.path.exists(token_dir):\n",
        "  os.makedirs(token_dir)\n",
        "tokenizer.save_model('HegelBERT')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7cKSDDgc0VF"
      },
      "source": [
        "**Load the trained tokenizer.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdjPEvVgONDn"
      },
      "outputs": [],
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./HegelBERT/vocab.json\",\n",
        "    \"./HegelBERT/merges.txt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JMjDmN3SwJX"
      },
      "outputs": [],
      "source": [
        "# Adding the start and end token.\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\",tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\",tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmQZtzWSc8s-"
      },
      "source": [
        "**Define model configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMX5pcsFTSPD",
        "outputId": "c91d797f-d0d6-4f0e-853c-5b81f8d49d86",
        "colab": {
          "referenced_widgets": [
            "fd5a25072468466ba35b3841b8ec5583"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd5a25072468466ba35b3841b8ec5583",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ojk6aAhHUJl8"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"./HegelBERT\",max_len=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6al76KCTUdcr",
        "outputId": "62edff03-1603-4c67-85f2-f8c3e63afac1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-27 04:59:08.957067: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRV7j_95UsJb",
        "outputId": "c308c99f-69ae-4920-c6f0-ea7cb05815b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RobertaForMaskedLM(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): RobertaLMHead(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft6_qzYrdUQH"
      },
      "source": [
        "# **Pretraining the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXvauvUxdhSR"
      },
      "source": [
        "**Building the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moEIEjHQUtl8",
        "outputId": "167349c2-3ebf-4cef-d10b-b0ef4d23fc9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 9.23 s, sys: 286 ms, total: 9.51 s\n",
            "Wall time: 3.27 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    file_path = \"./hegel.txt\",\n",
        "    block_size = 128,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rrQKQMudnx6"
      },
      "source": [
        "**Define the Data Collator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJP1fkvtVMbZ"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer = tokenizer,\n",
        "    mlm = True,\n",
        "    mlm_probability = 0.15,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf158XpBVjex"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK9P_U0aduGk"
      },
      "source": [
        "**Initializing the Trainer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQiUs_iIVr5X"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./HegelBERT\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgz6ZYPqWBXa"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMhD8Q1Pd0iE"
      },
      "source": [
        "**Pretraining**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "gLFgkOtHWPne",
        "outputId": "9800c8a9-3dd8-4b35-8cec-dd005befc41d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1587' max='1598' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1587/1598 54:36 < 00:22, 0.48 it/s, Epoch 0.99/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>7.083500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>6.434500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>6.258200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%time\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isMVjgwfeAZQ"
      },
      "source": [
        "**Saving the final model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyGDmftVWXTd",
        "outputId": "f9bad0b0-d3cb-45f6-8a98-87b93c56b88e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model...\n",
            "Model saved!\n"
          ]
        }
      ],
      "source": [
        "print(\"Saving model...\")\n",
        "trainer.save_model(\"./HegelBERT\")\n",
        "print(\"Model saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STZFuiToeGPD"
      },
      "source": [
        "**Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEUHUZDXbpG9",
        "outputId": "64ddb147-0173-4a52-ff89-3902ec94b17d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./HegelBERT\",\n",
        "    tokenizer=\"./HegelBERT\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z01rpr60fp7q",
        "outputId": "cf363e2e-3134-461d-e8f6-434c4df83179"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'score': 0.12893177568912506,\n",
              "  'token': 18,\n",
              "  'token_str': '.',\n",
              "  'sequence': 'To be independent. public opinion is the first formal condition of achieving anything great'},\n",
              " {'score': 0.09787289053201675,\n",
              "  'token': 16,\n",
              "  'token_str': ',',\n",
              "  'sequence': 'To be independent, public opinion is the first formal condition of achieving anything great'},\n",
              " {'score': 0.07722752541303635,\n",
              "  'token': 266,\n",
              "  'token_str': ' the',\n",
              "  'sequence': 'To be independent the public opinion is the first formal condition of achieving anything great'},\n",
              " {'score': 0.033616770058870316,\n",
              "  'token': 274,\n",
              "  'token_str': ' of',\n",
              "  'sequence': 'To be independent of public opinion is the first formal condition of achieving anything great'},\n",
              " {'score': 0.033488426357507706,\n",
              "  'token': 301,\n",
              "  'token_str': ' to',\n",
              "  'sequence': 'To be independent to public opinion is the first formal condition of achieving anything great'}]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fill_mask(\"To be independent <mask> public opinion is the first formal condition of achieving anything great\")\n",
        "\n",
        "# more epochs needed for optimized model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}