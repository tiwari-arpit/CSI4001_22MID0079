{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiwari-arpit/nlp/blob/main/SeNER_Hindi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PHGqiyd1sm6Y"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cluUA_urqDbK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig, AutoModelForTokenClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeJUAqKxrUxX"
      },
      "outputs": [],
      "source": [
        "model_name = \"arpit-tiwari/distilbert-finetuned-hindi-ner\"\n",
        "model_checkpoint = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laiG0DN4sE-C"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "lang = 'hi'\n",
        "data = load_dataset('ai4bharat/naamapadam',lang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FhlkdsFHU-r"
      },
      "outputs": [],
      "source": [
        "hindi_data = data\n",
        "hindi_data['train'] = data['train'].select(range(50000,100001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7pIqLXYsbUo"
      },
      "outputs": [],
      "source": [
        "tags = hindi_data['train'].features['ner_tags'].feature\n",
        "def create_tag_name(batch):\n",
        "  tag_name = {'ner_tags_str': [ tags.int2str(idx) for idx in batch['ner_tags']]}\n",
        "  return tag_name\n",
        "\n",
        "hindi_data = hindi_data.map(create_tag_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7Q1DE13thBn"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = hindi_data.map(tokenize_and_align_labels,batched=True,remove_columns=hindi_data['train'].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QarwTP-lpnNL"
      },
      "outputs": [],
      "source": [
        "class ArrowAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, window_size, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "        self.cls_attention = nn.MultiheadAttention(hidden_size, num_heads)\n",
        "        self.local_attention = nn.MultiheadAttention(hidden_size, num_heads)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
        "\n",
        "        cls_token = hidden_states[:, 0:1, :]\n",
        "        cls_token = cls_token.transpose(0, 1)\n",
        "        global_output, _ = self.cls_attention(\n",
        "            cls_token,\n",
        "            hidden_states.transpose(0, 1),\n",
        "            hidden_states.transpose(0, 1)\n",
        "        )\n",
        "        global_output = global_output.transpose(0, 1)\n",
        "\n",
        "        if seq_len > 1:\n",
        "            local_outputs = []\n",
        "            for i in range(1, seq_len):\n",
        "                start = max(1, i - self.window_size)\n",
        "                end = min(seq_len, i + self.window_size + 1)\n",
        "\n",
        "                window = hidden_states[:, start:end, :]\n",
        "                current_token = hidden_states[:, i:i+1, :]\n",
        "\n",
        "                current_token = current_token.transpose(0, 1)\n",
        "                window = window.transpose(0, 1)\n",
        "\n",
        "                local_output, _ = self.local_attention(\n",
        "                    current_token,\n",
        "                    window,\n",
        "                    window\n",
        "                )\n",
        "                local_output = local_output.transpose(0, 1)\n",
        "                local_outputs.append(local_output)\n",
        "\n",
        "            if local_outputs:\n",
        "                local_output = torch.cat(local_outputs, dim=1)\n",
        "                output = torch.cat([global_output, local_output], dim=1)\n",
        "            else:\n",
        "                output = global_output\n",
        "        else:\n",
        "            output = global_output\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKDb56R7qRV9"
      },
      "outputs": [],
      "source": [
        "class LogNScaling(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(LogNScaling, self).__init__()\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0 / (hidden_size ** 0.5)))\n",
        "\n",
        "    def forward(self, attention_scores):\n",
        "        return attention_scores * self.scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcf4VbTQqSC7"
      },
      "outputs": [],
      "source": [
        "class BiSPA(nn.Module):\n",
        "    def __init__(self, hidden_size, window_size, num_heads=8):\n",
        "        super(BiSPA, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.horizontal_attention = nn.MultiheadAttention(hidden_size, num_heads)\n",
        "        self.vertical_attention = nn.MultiheadAttention(hidden_size, num_heads)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
        "        hidden_states_t = hidden_states.transpose(0, 1)\n",
        "\n",
        "        # Horizontal attention\n",
        "        horizontal_outputs = []\n",
        "        for i in range(seq_len):\n",
        "            start = max(0, i - self.window_size)\n",
        "            end = min(seq_len, i + self.window_size + 1)\n",
        "            horizontal_input = hidden_states[start:end, :, :]\n",
        "            query = hidden_states[i:i+1, :, :]\n",
        "\n",
        "            horizontal_output, _ = self.horizontal_attention(\n",
        "                query,\n",
        "                horizontal_input,\n",
        "                horizontal_input\n",
        "            )\n",
        "            horizontal_outputs.append(horizontal_output)\n",
        "\n",
        "        horizontal_output = torch.cat(horizontal_outputs, dim=0)\n",
        "\n",
        "        # Vertical attention\n",
        "        vertical_outputs = []\n",
        "        for i in range(seq_len):\n",
        "            start = max(0, i - self.window_size)\n",
        "            end = min(seq_len, i + self.window_size + 1)\n",
        "            query = hidden_states_t[i:i+1, :, :]\n",
        "            vertical_input = hidden_states[start:end, :, :]\n",
        "            vertical_output, _ = self.vertical_attention(\n",
        "                query,\n",
        "                vertical_input,\n",
        "                vertical_input\n",
        "            )\n",
        "            vertical_outputs.append(vertical_output)\n",
        "\n",
        "        vertical_output = torch.cat(vertical_outputs, dim=0)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined_output = torch.cat([horizontal_output, vertical_output], dim=-1)\n",
        "        output = self.mlp(combined_output)\n",
        "        output = output.transpose(0, 1)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6KpG1b5qW9q"
      },
      "outputs": [],
      "source": [
        "class SeNER(nn.Module):\n",
        "    def __init__(self, model_name, num_labels, window_size=128):\n",
        "        super(SeNER, self).__init__()\n",
        "        self.config = AutoConfig.from_pretrained(model_name)\n",
        "        self.base_model = AutoModel.from_pretrained(model_name, config=self.config)\n",
        "        self.num_labels = num_labels\n",
        "        self.arrow_attention = ArrowAttention(self.config.hidden_size, window_size)\n",
        "        self.log_n_scaling = LogNScaling(self.config.hidden_size)\n",
        "        self.bispa = BiSPA(self.config.hidden_size, window_size)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "        self.dropout = nn.Dropout(self.config.hidden_size_dropout if hasattr(self.config, 'hidden_size_dropout') else 0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "\n",
        "        attended_output = self.arrow_attention(sequence_output)\n",
        "\n",
        "        if attended_output.size(1) > 0:\n",
        "            cls_token = attended_output[:, 0:1, :]\n",
        "            scaled_cls = self.log_n_scaling(cls_token)\n",
        "\n",
        "            if attended_output.size(1) > 1:\n",
        "                attended_output = torch.cat([scaled_cls, attended_output[:, 1:, :]], dim=1)\n",
        "            else:\n",
        "                attended_output = scaled_cls\n",
        "\n",
        "        sequence_output = self.bispa(attended_output)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            active_loss = labels.view(-1) != -100\n",
        "            active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
        "            active_labels = labels.view(-1)[active_loss]\n",
        "            loss = loss_fct(active_logits, active_labels)\n",
        "\n",
        "        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HclmJ5blxtEl"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_colator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCEX8Zm2XhVq",
        "outputId": "13b4ef13-0d83-4b9a-b044-585c3694fb3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36WHbdSNIZE0",
        "outputId": "6d7371fe-3484-4a8e-cf86-b380cf27d12c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['content']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# if not os.path.exists(extract_dir):\n",
        "#     os.makedirs(extract_dir)\n",
        "\n",
        "with zipfile.ZipFile(\"/content/MyDrive/MyDrive/SeNER.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"extract_dir\")\n",
        "\n",
        "os.listdir(\"/content/extract_dir/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s4NGHr1beFI",
        "outputId": "63b4e859-f09e-40b0-dddb-33db99aeb145"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tokenizer_config.json',\n",
              " 'special_tokens_map.json',\n",
              " 'vocab.txt',\n",
              " 'model.safetensors',\n",
              " 'training_args.bin',\n",
              " 'tokenizer.json']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.listdir(\"drive/MyDrive/nlp/SeNER/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdnnDXosqt3T"
      },
      "outputs": [],
      "source": [
        "num_labels = len(tags.names)\n",
        "model = SeNER(model_name, num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6z0noFybu0B",
        "outputId": "9c0f233e-f209-495d-8291-bd220f611b78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SeNER(\n",
              "  (base_model): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (arrow_attention): ArrowAttention(\n",
              "    (cls_attention): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "    )\n",
              "    (local_attention): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (log_n_scaling): LogNScaling()\n",
              "  (bispa): BiSPA(\n",
              "    (horizontal_attention): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "    )\n",
              "    (vertical_attention): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "    )\n",
              "    (mlp): Sequential(\n",
              "      (0): Linear(in_features=1536, out_features=768, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=768, out_features=768, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from safetensors.torch import load_file\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "state_dict = load_file(\"drive/MyDrive/nlp/SeNER/model.safetensors\")\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMB78Qk9qudl"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./SeNER_Hindi\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtIzqx1ZqulW"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    data_collator = data_colator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGaO0vaLqzap"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_DqYoq17Brj"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"./SeNER\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EQoFPWC_xuV"
      },
      "outputs": [],
      "source": [
        "def predict_entities(model, tokenizer, text, id2label=None):\n",
        "    inputs = tokenizer(text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs[\"logits\"]\n",
        "    predictions = torch.argmax(logits, dim=-1)[0].cpu().numpy()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    offset_mapping = offset_mapping[0].cpu().numpy()\n",
        "\n",
        "    word_entities = []\n",
        "    previous_word = None\n",
        "    current_word = \"\"\n",
        "    current_label = None\n",
        "\n",
        "    for token, pred, offset in zip(tokens, predictions, offset_mapping):\n",
        "        if offset[0] == 0 and offset[1] == 0:\n",
        "            continue\n",
        "\n",
        "        start, end = offset\n",
        "        word = text[start:end]\n",
        "\n",
        "        if token.startswith(\"##\"):\n",
        "            current_word += word\n",
        "        else:\n",
        "            if current_word:\n",
        "                label_name = id2label[current_label] if id2label else current_label\n",
        "                word_entities.append((current_word, label_name))\n",
        "            current_word = word\n",
        "            current_label = pred\n",
        "\n",
        "    if current_word:\n",
        "        label_name = id2label[current_label] if id2label else current_label\n",
        "        word_entities.append((current_word, label_name))\n",
        "\n",
        "    return word_entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0mQ3KUm6FVl",
        "outputId": "791de7f6-6af5-494f-8b70-c0517617c87a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input text: अमेरिका की ओर से लगाए गए 104 फ़ीसदी टैरिफ़ पर चीन के राष्ट्रपति शी जिंगपिंग ने अपनी प्रतिक्रिया दी है. शी जिनपिंग ने कहा है कि अमेरिका के टैरिफ़ का सामना करने के लिए चीन को अपने पड़ोसी देशों के साथ रिश्ते मज़बूत करने चाहिए.नए अमेरिकी टैरिफ़ लागू होने के बाद यह पहली बार है जब शी जिंगपिंग ने टैरिफ़ पर बात की है.उन्होंने ये भी कहा कि एशियाई देशों को मिलकर एक बेहतर भविष्य के लिए काम करना होगा. चीन के राष्ट्रपति शी जिनपिंग का ये बयान दक्षिण-पूर्व एशियाई देशों के संगठन आसियान की हालिया टिप्पणी के समान है. आसियान ने भी अपने क्षेत्र में आर्थिक सहयोग बढ़ाने की अपील की थी.वियतनाम, कंबोडिया और इंडोनेशिया पर अमेरिकी टैरिफ़ का सबसे ज़्यादा प्रभाव पड़ा है.\n",
            "Predictions:\n",
            "अमेरिका: B-LOC\n",
            "की: O\n",
            "ओर: O\n",
            "से: O\n",
            "लगाए: O\n",
            "गए: O\n",
            "104: O\n",
            "फ़ीसदी: O\n",
            "टैरिफ़: O\n",
            "पर: O\n",
            "चीन: O\n",
            "के: O\n",
            "राष्ट्रपति: O\n",
            "शी: B-PER\n",
            "जिंगपिंग: I-PER\n",
            "ने: O\n",
            "अपनी: O\n",
            "प्रतिक्रिया: O\n",
            "दी: O\n",
            "है: O\n",
            ".: O\n",
            "शी: B-PER\n",
            "जिनपिंग: I-PER\n",
            "ने: O\n",
            "कहा: O\n",
            "है: O\n",
            "कि: O\n",
            "अमेरिका: O\n",
            "के: O\n",
            "टैरिफ़: O\n",
            "का: O\n",
            "सामना: O\n",
            "करने: O\n",
            "के: O\n",
            "लिए: O\n",
            "चीन: B-LOC\n",
            "को: O\n",
            "अपने: O\n",
            "पड़ोसी: O\n",
            "देशों: O\n",
            "के: O\n",
            "साथ: O\n",
            "रिश्ते: O\n",
            "मज़बूत: O\n",
            "करने: O\n",
            "चाहिए: O\n",
            ".: O\n",
            "नए: O\n",
            "अमेरिकी: O\n",
            "टैरिफ़: O\n",
            "लागू: O\n",
            "होने: O\n",
            "के: O\n",
            "बाद: O\n",
            "यह: O\n",
            "पहली: O\n",
            "बार: O\n",
            "है: O\n",
            "जब: O\n",
            "शी: B-PER\n",
            "जिंगपिंग: I-PER\n",
            "ने: O\n",
            "टैरिफ़: O\n",
            "पर: O\n",
            "बात: O\n",
            "की: O\n",
            "है: O\n",
            ".: O\n",
            "उन्होंने: O\n",
            "ये: O\n",
            "भी: O\n",
            "कहा: O\n",
            "कि: O\n",
            "एशियाई: O\n",
            "देशों: O\n",
            "को: O\n",
            "मिलकर: O\n",
            "एक: O\n",
            "बेहतर: O\n",
            "भविष्य: O\n",
            "के: O\n",
            "लिए: O\n",
            "काम: O\n",
            "करना: O\n",
            "होगा: O\n",
            ".: O\n",
            "चीन: B-LOC\n",
            "के: O\n",
            "राष्ट्रपति: O\n",
            "शी: B-PER\n",
            "जिनपिंग: I-PER\n",
            "का: O\n",
            "ये: O\n",
            "बयान: O\n",
            "दक्षिण: O\n",
            "-: O\n",
            "पूर्व: O\n",
            "एशियाई: O\n",
            "देशों: O\n",
            "के: O\n",
            "संगठन: O\n",
            "आसियान: O\n",
            "की: O\n",
            "हालिया: O\n",
            "टिप्पणी: O\n",
            "के: O\n",
            "समान: O\n",
            "है: O\n",
            ".: O\n",
            "आसियान: O\n",
            "ने: O\n",
            "भी: O\n",
            "अपने: O\n",
            "क्षेत्र: O\n",
            "में: O\n",
            "आर्थिक: O\n",
            "सहयोग: O\n",
            "बढ़ाने: O\n",
            "की: O\n",
            "अपील: O\n",
            "की: O\n",
            "थी: O\n",
            ".: O\n",
            "वियतनाम: O\n",
            ",: O\n",
            "कंबोडिया: O\n",
            "और: O\n",
            "इंडोनेशिया: O\n",
            "पर: O\n",
            "अमेरिकी: O\n",
            "टैरिफ़: O\n",
            "का: O\n",
            "सबसे: O\n",
            "ज़्यादा: O\n",
            "प्रभाव: O\n",
            "पड़ा: O\n",
            "है: O\n",
            ".: O\n"
          ]
        }
      ],
      "source": [
        "#text = \"इटली की पीएम जॉर्जिया मेलोनी ने पूरी दुनिया के लेफ्टिस्ट लीडर्स को पाखंडी बताया है। उन्होंने कहा कि दुनियाभर में मोदी, ट्रम्प और मेरे जैसे दक्षिणपंथी नेताओं के उभरने से सारे लेफ्टिस्ट नेता परेशान हो गए हैं|\"\n",
        "# text = \"भारत की नरेंद्र मोदी सरकार अमेरिकी राष्ट्रपति डॉनल्ड ट्रंप द्वारा लगाए गए 27% आयात शुल्क के खिलाफ किसी भी जवाबी कार्रवाई की योजना फिलहाल नहीं बना रही है. अंतरराष्ट्रीय न्यूज़ एजेंसी रॉयटर्स ने एक वरिष्ठ सरकारी अधिकारी के हवाले से लिखा है कि दिल्ली और वॉशिंगटन डीसी के बीच व्यापार समझौते पर बातचीत जारी है, और इसी कारण भारत संयम बरत रहा है. रिपोर्ट के मुताबिक अधिकारी ने बताया कि केंद्र सरकार डॉनल्ड ट्रंप के टैरिफ ऑर्डर में दिए गए उस प्रावधान का ‘अध्ययन’ कर रही है जिसमें कहा गया है कि जो देश व्यापार को बराबरी पर लाने के लिए सार्थक कदम उठाते हैं, उन्हें छूट दी जा सकती है.\"\n",
        "text = \"अमेरिका की ओर से लगाए गए 104 फ़ीसदी टैरिफ़ पर चीन के राष्ट्रपति शी जिंगपिंग ने अपनी प्रतिक्रिया दी है. \\\n",
        "शी जिनपिंग ने कहा है कि अमेरिका के टैरिफ़ का सामना करने के लिए चीन को अपने पड़ोसी देशों के साथ रिश्ते मज़बूत करने चाहिए.\\\n",
        "नए अमेरिकी टैरिफ़ लागू होने के बाद यह पहली बार है जब शी जिंगपिंग ने टैरिफ़ पर बात की है.\\\n",
        "उन्होंने ये भी कहा कि एशियाई देशों को मिलकर एक बेहतर भविष्य के लिए काम करना होगा. \\\n",
        "चीन के राष्ट्रपति शी जिनपिंग का ये बयान दक्षिण-पूर्व एशियाई देशों के संगठन आसियान की हालिया टिप्पणी के समान है. आसियान ने भी अपने क्षेत्र में आर्थिक सहयोग बढ़ाने की अपील की थी.\\\n",
        "वियतनाम, कंबोडिया और इंडोनेशिया पर अमेरिकी टैरिफ़ का सबसे ज़्यादा प्रभाव पड़ा है.\"\n",
        "predictions = predict_entities(model, tokenizer, text)\n",
        "\n",
        "print(\"Input text:\", text)\n",
        "print(\"Predictions:\")\n",
        "for token, pred in predictions:\n",
        "    print(f\"{token}: {tags.int2str(int(pred))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRaRBYDJ_7fc",
        "outputId": "4dd079f5-9811-4968-c040-5b2233373883"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nv4kL9mA4yG"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kWadNkwBzU6",
        "outputId": "22ff2b5d-8975-40b1-b537-0e3c63a3eab4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XvtpkVqJz6f",
        "outputId": "9d46ba08-f422-4664-e07b-59fa9a7da883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/SeNER/ (stored 0%)\n",
            "  adding: content/SeNER/special_tokens_map.json (deflated 80%)\n",
            "  adding: content/SeNER/training_args.bin (deflated 52%)\n",
            "  adding: content/SeNER/tokenizer_config.json (deflated 74%)\n",
            "  adding: content/SeNER/tokenizer.json (deflated 70%)\n",
            "  adding: content/SeNER/model.safetensors (deflated 8%)\n",
            "  adding: content/SeNER/vocab.txt (deflated 49%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r SeNER.zip /content/SeNER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw9wS7xPJQ7r"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('SeNER.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}